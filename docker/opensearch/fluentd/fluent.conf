# Fluentd configuration for Backstage Cloud Sandbox
# Collects logs from Docker containers and forwards to OpenSearch + MinIO archive

# ============================================
# INPUT SOURCES
# ============================================

# Docker container logs via Fluentd logging driver
<source>
  @type forward
  port 24224
  bind 0.0.0.0
</source>

# HTTP input for direct log shipping and health checks
<source>
  @type http
  port 9880
  bind 0.0.0.0
  body_size_limit 32m
  keepalive_timeout 10s
</source>

# Monitor agent for health checks
<source>
  @type monitor_agent
  bind 0.0.0.0
  port 24220
</source>

# ============================================
# FILTERS - Parse and enrich logs
# ============================================

# Add container metadata
<filter docker.**>
  @type record_transformer
  <record>
    container_name ${tag_parts[1]}
    log_type docker
    environment development
    hostname "#{Socket.gethostname}"
  </record>
</filter>

# Parse JSON logs from Backstage (gracefully handle non-JSON)
<filter docker.backstage-app>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field true
  emit_invalid_record_to_error false
  <parse>
    @type json
    time_key @timestamp
    time_format %Y-%m-%dT%H:%M:%S.%LZ
  </parse>
</filter>

# Parse nginx access logs (skip non-matching lines like startup messages)
<filter docker.backstage-nginx>
  @type parser
  key_name log
  reserve_data true
  remove_key_name_field false
  emit_invalid_record_to_error false
  <parse>
    @type regexp
    expression /^(?<remote_addr>[^ ]*) - (?<remote_user>[^ ]*) \[(?<time_local>[^\]]*)\] "(?<method>\S+)(?: +(?<path>[^\"]*?)(?: +\S*)?)?" (?<status>[^ ]*) (?<body_bytes_sent>[^ ]*)(?: "(?<http_referer>[^\"]*)" "(?<http_user_agent>[^\"]*)")?/
    time_key time_local
    time_format %d/%b/%Y:%H:%M:%S %z
  </parse>
</filter>

# Categorize log levels
<filter docker.**>
  @type record_transformer
  enable_ruby true
  <record>
    level ${record["level"] || (record["log"] =~ /error|fail|exception/i ? "error" : (record["log"] =~ /warn/i ? "warn" : "info"))}
    @timestamp ${time.strftime('%Y-%m-%dT%H:%M:%S.%LZ')}
  </record>
</filter>

# ============================================
# FILTERS - Reduce log volume for OpenSearch
# ============================================

# Drop health check and noisy internal requests from Backstage
<filter docker.backstage-app>
  @type grep
  <exclude>
    key url
    pattern /^\/(health|healthcheck|\.well-known)|\/jwks\.json|\/auth\/v1\/jwks/
  </exclude>
</filter>

# Drop incomingRequest logs (HTTP access logs) - they're too noisy
# Keep errors, warnings, and all other log types
<filter docker.backstage-app>
  @type grep
  <exclude>
    key type
    pattern /^incomingRequest$/
  </exclude>
</filter>

# Drop nginx health checks
<filter docker.backstage-nginx>
  @type grep
  <exclude>
    key path
    pattern /^\/(health|healthcheck)/
  </exclude>
</filter>

# ============================================
# OUTPUT - Backstage logs (filtered, low volume)
# ============================================

<match docker.backstage-app>
  @type copy

  # Real-time: OpenSearch (only important logs)
  <store>
    @type opensearch
    host opensearch
    port 9200
    scheme http
    ssl_verify false
    logstash_format true
    logstash_prefix backstage
    logstash_dateformat %Y.%m.%d
    include_tag_key true
    tag_key @log_name
    suppress_type_name true
    reload_connections false
    reconnect_on_error true
    <buffer tag>
      @type file
      path /fluentd/log/opensearch-backstage
      flush_interval 5s
      chunk_limit_size 4MB
      queue_limit_length 32
      retry_type exponential_backoff
      retry_wait 5s
      retry_forever true
      compress gzip
    </buffer>
  </store>

  # Archive: MinIO S3
  <store>
    @type s3
    aws_key_id backstage
    aws_sec_key backstage123
    s3_bucket log-archive
    s3_endpoint http://minio:9000
    s3_region us-east-1
    force_path_style true
    path backstage/%Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format %Y%m%d-%H
    <buffer time>
      @type file
      path /fluentd/log/s3-backstage
      timekey 5m
      timekey_wait 1m
      timekey_use_utc true
      chunk_limit_size 64m
    </buffer>
    <format>
      @type json
    </format>
  </store>
</match>

# ============================================
# OUTPUT - Nginx logs
# ============================================

<match docker.backstage-nginx>
  @type copy

  <store>
    @type opensearch
    host opensearch
    port 9200
    scheme http
    ssl_verify false
    logstash_format true
    logstash_prefix nginx
    logstash_dateformat %Y.%m.%d
    include_tag_key true
    tag_key @log_name
    suppress_type_name true
    reload_connections false
    reconnect_on_error true
    <buffer tag>
      @type file
      path /fluentd/log/opensearch-nginx
      flush_interval 10s
      chunk_limit_size 4MB
      queue_limit_length 32
      retry_type exponential_backoff
      retry_wait 5s
      retry_forever true
      compress gzip
    </buffer>
  </store>

  <store>
    @type s3
    aws_key_id backstage
    aws_sec_key backstage123
    s3_bucket log-archive
    s3_endpoint http://minio:9000
    s3_region us-east-1
    force_path_style true
    path nginx/%Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format %Y%m%d-%H
    <buffer time>
      @type file
      path /fluentd/log/s3-nginx
      timekey 5m
      timekey_wait 1m
      timekey_use_utc true
      chunk_limit_size 64m
    </buffer>
    <format>
      @type json
    </format>
  </store>
</match>

# ============================================
# OUTPUT - Infrastructure logs (postgres, redis, minio)
# ============================================

<match docker.backstage-{postgres,redis,minio,minio-init}>
  @type copy

  <store>
    @type opensearch
    host opensearch
    port 9200
    scheme http
    ssl_verify false
    logstash_format true
    logstash_prefix infrastructure
    logstash_dateformat %Y.%m.%d
    include_tag_key true
    tag_key @log_name
    suppress_type_name true
    reload_connections false
    reconnect_on_error true
    <buffer tag>
      @type file
      path /fluentd/log/opensearch-infrastructure
      flush_interval 15s
      chunk_limit_size 4MB
      queue_limit_length 16
      retry_type exponential_backoff
      retry_wait 5s
      retry_forever true
      compress gzip
    </buffer>
  </store>

  <store>
    @type s3
    aws_key_id backstage
    aws_sec_key backstage123
    s3_bucket log-archive
    s3_endpoint http://minio:9000
    s3_region us-east-1
    force_path_style true
    path infrastructure/%Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format %Y%m%d-%H
    <buffer time>
      @type file
      path /fluentd/log/s3-infrastructure
      timekey 5m
      timekey_wait 1m
      timekey_use_utc true
      chunk_limit_size 64m
    </buffer>
    <format>
      @type json
    </format>
  </store>
</match>

# ============================================
# OUTPUT - All other Docker logs
# ============================================

<match docker.**>
  @type copy

  <store>
    @type opensearch
    host opensearch
    port 9200
    scheme http
    ssl_verify false
    logstash_format true
    logstash_prefix logs
    logstash_dateformat %Y.%m.%d
    include_tag_key true
    tag_key @log_name
    suppress_type_name true
    reload_connections false
    reconnect_on_error true
    <buffer tag>
      @type file
      path /fluentd/log/opensearch-misc
      flush_interval 15s
      chunk_limit_size 4MB
      queue_limit_length 16
      retry_type exponential_backoff
      retry_wait 5s
      retry_forever true
      compress gzip
    </buffer>
  </store>

  <store>
    @type s3
    aws_key_id backstage
    aws_sec_key backstage123
    s3_bucket log-archive
    s3_endpoint http://minio:9000
    s3_region us-east-1
    force_path_style true
    path misc/%Y/%m/%d/
    s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension}
    time_slice_format %Y%m%d-%H
    <buffer time>
      @type file
      path /fluentd/log/s3-misc
      timekey 5m
      timekey_wait 1m
      timekey_use_utc true
      chunk_limit_size 64m
    </buffer>
    <format>
      @type json
    </format>
  </store>
</match>

# Drop any remaining logs (prevents feedback loops)
<match **>
  @type null
</match>
